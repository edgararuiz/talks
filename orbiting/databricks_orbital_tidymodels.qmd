---
title: Databricks + Tididymodels + Orbital
subtitle: Simple use case example
author: Edgar Ruiz - Posit
date: 2025-05-22
format: html
editor: visual
execute:
  eval: false
toc: true  
toc-expand: true
---

```{r}
#| include: false
library(tidymodels)
library(tidyverse)
library(dbplyr)
options("odbc.no_config_override"=FALSE)
```

## Use case

Using loan data, we want to use a model that estimates an appropriate interest 
rate, and then use that model to find out if the interest rate for a given loan
may have been too high. The loan data is in a table located in the Databricks 
Unity Catalog. The ultimate objective of the project, is to have it check on 
a daily basis to see what loans may an issue. 

## Approach

*"Fit small, predict big"*

To make it as close to a 'real-life' scenario, we will download a sample of the 
table into our R session, fit a model using a Tidymodels, and
then use Orbital to translate the steps and estimates into a SQL statement. 
Finally, we will use that SQL statement as the base to compare the current 
interest against the prediction, and download the loans that had a large 
difference. Thanks to the integrated environment in Databricks, the resulting
SQL statement will be saved in the Databricks Workspace, and used to run
on a schedule via a [Databricks Job](https://docs.databricks.com/aws/en/jobs/).

::: {#fig-diagram}
```{mermaid}
flowchart LR
  A[1-Full Table] --Download--> B(2-Sample) 
  B--Tidymodels fit-->C(3-Model)
  C--Orbital parse-->D(4-SQL)
  D--Automate-->E(5-Job)
  E--Predict-->A
```

Diagram of the approach used for this use case
:::

## Download sample

1.  The [`odbc::databricks()`](https://odbc.r-dbi.org/reference/databricks.html) 
function provides lots of convenient features that handle essentially everything
needed to establish a connection to Databricks. The only thing to provide is 
the `httpPath` in order to succeed. The connection is established via `DBI`.

    ```{r}
    library(DBI)
    
    con <- dbConnect(
      drv = odbc::databricks(), 
      httpPath = "/sql/1.0/warehouses/b71952ebceb705ce"
      )
    ```

2.  Because they will be used in multiple locations in this example, the `catalog`,
`schema`, and `table` name are loaded to variables

    ```{r}
    catalog <- "sol_eng_demo_nickp"
    schema <- "end-to-end"
    table <- "loans_full_schema"
    ```

3.  Sample the database table using the `TABLESAMPLE` SQL function. `REPEATABLE` 
is used to aid with reproducibility.  The `glue_sql()` function is used to 
compile the SQL statement.

    ```{r}
    library(glue)
    
    sample_sql <- glue_sql(
      "SELECT * ", 
      "FROM ",
      "{`catalog`}.{`schema`}.{`table`}",
      "TABLESAMPLE (100 ROWS) REPEATABLE (999)",
      .con = con
      )
    
    sample_sql
    ```

4.  The sample is downloaded by executing the SQL statement via `dbGetQuery()`.
    
    ```{r}
    sample_lending <- dbGetQuery(
      conn = con, 
      statement = sample_sql
      )
    ```


## Fit locally

1.  Load `tidymodels` and set the seed

    ```{r}
    library(tidymodels)
    set.seed(999)
    ```

2.  Currently, **certain fields are downloaded from Databricks as Integer 64 type.
These are not supported by R in general**. The easiest solution is to convert them
to double. This needs to only be done in the local copy, since goal is to have
a SQL statement that will run inside Databricks. To make the data transformation
easy, we will use `dplyr`.
    
    ```{r}
    library(dplyr)
    
    sample_lending <- sample_lending |> 
      mutate(
        total_credit_lines = as.double(total_credit_lines),
        loan_amount = as.double(loan_amount),
        term = as.double(term)
        )
    ```

3.  Split the data into training and testing. 

    ```{r}
    split_lending <- initial_split(sample_lending)
    
    lending_training <- training(split_lending)
    ```

4.  Create a `recipe` that defines the predictors and outcome fields, and includes
a normalization step. For this example, we will use the `annual_income`, 
`total_credit_lines`, `loan_amount` and `term` fields for predictors.

    ```{r}
    rec_lending <- recipe(
      interest_rate ~ annual_income + total_credit_lines + loan_amount + term, 
      data = lending_training
      ) |> 
      step_normalize(all_numeric_predictors())
    ```

5.  Define a linear regression model spec.

    ```{r}
    lm_spec <- linear_reg()
    ```

6.  Create the workflow by combining the recipe and the defined model spec.

    ```{r}
    wf_spec <- workflow(rec_lending, lm_spec)
    ```

7.  Fit the workflow using the training data, and preview
    
    ```{r}
    wf_fit <- fit(wf_spec, lending_training)
    
    wf_fit
    ```


## Convert to SQL using Orbital

1.  Load and use `orbital` to read the fitted workflow. In Databricks, names
with dots (".") are not acceptable, and `.pred` is the default name that 
`orbital` gives the prediction. To fix, use the `prefix` to override. 

    ```{r}
    library(orbital)
    
    lending_orbital <-  orbital(wf_fit, prefix = "pred")
    
    lending_orbital
    ```

2.  Load `dbplyr`, and use `tbl` to create a reference to the lending table
in the R session. To pass the fully qualified name, we can use `glue` and `I()`.

    ```{r}
    library(dbplyr)
    
    tbl_lending <- tbl(con, I(glue("{catalog}.`{schema}`.{table}")))
    
    tbl_lending
    ```

3.  In order to make the predictions part of a larger set of fields returned 
by the final query, the `orbital_inline()` function is used. This allows for
it to be passed inside a `dplyr` `mutate()` call. `orbital_inline()` will
modify the predictor fields based on the steps from the recipe, which in the example's
case, is the normalization step. A quick way to retain the original values, if they
are to be part of the final result, is to simply create copies of them via 
`mutate()`. Finally, since it is not necessary to return all of the fields,
we reduce them via a `select()` call. 

    ```{r}
    tbl_prep <- tbl_lending |> 
      mutate(o_annual_income = annual_income, 
             o_total_credit_lines = total_credit_lines,
             o_loan_amount = loan_amount, 
             o_term = term
             ) |> 
      mutate(!!! orbital_inline(lending_orbital)) |> 
      select(
        pred, interest_rate, emp_title, balance, application_type,
        o_annual_income, o_total_credit_lines, o_loan_amount, o_term
      )
    ```

4.  Preview the top rows from the initial transformations.

    ```{r}
    tbl_prep |> 
      head()
    ```

5.  An additional step is added to only keep the rows that have an current
interest rate is 15 points higher than the prediction.
    
    ```{r}
    tbl_final <- tbl_prep |> 
      filter(interest_rate - pred > 15, pred > 0)
    ```

6.  Preview the results
    
    ```{r}
    tbl_final
    ```

7.  Preview the actual SQL that will be sent using `show_query()`.

    ```{r}
    tbl_final |> 
      show_query()
    ```

8.  `show_query()` is mostly geared towards having a nice output to the R 
console. To capture the SQL in a variable, use `remote_query()`.

    ```{r}
    final_sql <- remote_query(tbl_final)
    ```

9.  As a way to confirm that the SQL will run as returned by `remote_query`,
use `dbQuery()` to run the statement against the database.
    
    ```{r}
    res <- dbGetQuery(con, final_sql)
    head(res)
    ```
    
    
```{r}
library(reticulate)


py_require("databricks.sdk")

db_sdk <- import("databricks.sdk")

w <- db_sdk$WorkspaceClient()

srcs <- w$data_sources$list()

warehouse_id <- srcs[[1]]$warehouse_id

db_request_query <- db_sdk$service$sql$CreateQueryRequestQuery(
  query_text = final_sql,
  catalog = catalog,
  schema = schema,
  display_name = "Interest rate differences",
  warehouse_id = warehouse_id,
  description = "Find differences in interest rate"
)

new_query <- w$queries$create(query = db_request_query)


```





    

