---
title: Databricks + Tididymodels + Orbital
subtitle: Simple use case example
author: Edgar Ruiz - Posit
date: 2025-05-22
format: html
editor: visual
execute:
  eval: false
toc: true  
toc-expand: true
---

```{r}
#| include: false
library(tidymodels)
library(tidyverse)
library(dbplyr)
options("odbc.no_config_override"=FALSE)
```

## Use case

Using loan data, we want to use a model that estimates an appropriate interest 
rate, and then use that model to find out if the interest rate for a given loan
may have been too high. The loan data is in a table located in the Databricks 
Unity Catalog. The ultimate objective of the project, is to have it check on 
a daily basis to see what loans may an issue. 

## Approach

*"Fit small, predict big"*

To make it as close to a 'real-life' scenario, we will download a sample of the 
table into our Python session, fit a model using a Scikit Learn pipeline, and
then use Orbital to translate the steps and estimates into a SQL statement. 
Finally, we will use that SQL statement as the base to compare the current 
interest against the prediction, and download the loans that had a large 
difference. Thanks to the integrated environment in Databricks, the resulting
SQL statement will be saved in the Databricks Workspace, and used to run
on a schedule via a [Databricks Job](https://docs.databricks.com/aws/en/jobs/).

::: {#fig-diagram}
```{mermaid}
flowchart LR
  A[1-Full Table] --Download--> B(2-Sample) 
  B--Scikit Learn fit-->C(3-Model)
  C--Orbital parse-->D(4-SQL)
  D--Automate-->E(5-Job)
  E--Predict-->A
```

Diagram of the approach used for this use case
:::

## Download sample

1.  The [`odbc::databricks()`](https://odbc.r-dbi.org/reference/databricks.html) 
function provides lots of convenient features that handle essentially everything
needed to establish a connection to Databricks. The only thing to provide is 
the `httpPath` in order to succeed. The connection is established via `DBI`.

    ```{r}
    library(DBI)
    
    con <- dbConnect(
      drv = odbc::databricks(), 
      httpPath = "/sql/1.0/warehouses/b71952ebceb705ce"
      )
    ```

2.  Because they will be used in multiple locations in this example, the `catalog`,
`schema`, and `table` name are loaded to variables

    ```{r}
    catalog <- "sol_eng_demo_nickp"
    schema <- "end-to-end"
    table <- "loans_full_schema"
    ```

3.  Sample the database table using the `TABLESAMPLE` SQL function. `REPEATABLE` 
is used to aid with reproducibility.  The `glue_sql()` function is used to 
compile the SQL statement.

    ```{r}
    library(glue)
    
    sample_sql <- glue_sql(
      "SELECT * ", 
      "FROM ",
      "{`catalog`}.{`schema`}.{`table`}",
      "TABLESAMPLE (100 ROWS) REPEATABLE (999)",
      .con = con
      )
    
    sample_sql
    ```

4.  The sample is downloaded by executing the SQL statement via `dbGetQuery()`.
    
    ```{r}
    sample_lending <- dbGetQuery(
      conn = con, 
      statement = sample_sql
      )
    ```


## Fit locally

1.  Load `tidymodels` and set the seed

    ```{r}
    library(tidymodels)
    set.seed(999)
    ```

2.  Currently, **certain fields are downloaded from Databricks as Integer 64 type.
These are not supported by R in general**. The easiest solution is to convert them
to double. This needs to only be done in the local copy, since goal is to have
a SQL statement that will run inside Databricks. To make the data transformation
easy, we will use `dplyr`.
    
    ```{r}
    library(dplyr)
    
    sample_lending <- sample_lending |> 
      mutate(
        total_credit_lines = as.double(total_credit_lines),
        loan_amount = as.double(loan_amount),
        term = as.double(term)
        )
    ```

3.  Split the data into training and testing. 

    ```{r}
    split_lending <- initial_split(sample_lending)
    
    lending_training <- training(split_lending)
    ```

4.  Create a `recipe` that defines the predictors and outcome fields, and includes
a normalization step. For this example, we will use the `annual_income`, 
`total_credit_lines`, `loan_amount` and `term` fields for predictors.

    ```{r}
    rec_lending <- recipe(
      interest_rate ~ annual_income + total_credit_lines + loan_amount + term, 
      data = lending_training
      ) |> 
      step_normalize(all_numeric_predictors())
    ```

5.  Define a linear regression model spec.

    ```{r}
    lm_spec <- linear_reg()
    ```

6.  Create the workflow by combining the recipe and the defined model spec.

    ```{r}
    wf_spec <- workflow(rec_lending, lm_spec)
    ```

7.  Fit the workflow using the training data, and preview
    
    ```{r}
    wf_fit <- fit(wf_spec, lending_training)
    
    wf_fit
    ```


## Convert to SQL using Orbital

```{r}
library(orbital)

lending_orbital <-  orbital(wf_fit, prefix = "pred")

lending_orbital
```

```{r}
library(dplyr)
library(dbplyr)

tbl_lending <- tbl(con, I(glue("{catalog}.`{schema}`.{table}")))

tbl_lending
```



```{r}
tbl_prep <- tbl_lending |> 
  mutate(o_annual_income = annual_income, 
         o_total_credit_lines = total_credit_lines,
         o_loan_amount = loan_amount, 
         o_term = term
         ) |> 
  mutate(!!! orbital_inline(lending_orbital)) |> 
  select(
    pred, interest_rate, emp_title, balance, application_type,
    o_annual_income, o_total_credit_lines, o_loan_amount, o_term
  ) 

tbl_prep |> 
  head()
```

```{r}
tbl_final <- tbl_prep |> 
  filter(interest_rate - pred > 15)
```

```{r}
tbl_final
```


```{r}
tbl_final |> 
  show_query()
```

```{r}
final_sql <- remote_query(tbl_final)

final_sql
```

```{r}
dbGetQuery(con, final_sql)
```

