---
title: Databricks + Scikit Learn + Orbital
subtitle: Simple use case example
author: Edgar Ruiz - Posit
date: 2025-05-20
format: html
editor: visual
execute:
  eval: true
toc: true  
toc-expand: true
---

## Use case

Using loan data, we want to use a model that estimates an appropriate interest 
rate, and then use that model to find out if the interest rate for a given loan
may have been too high. The loan data is in a table located in Databricks. 

## Approach

*"Fit small, predict big"*

To make it as close to a 'real-life' scenario, we will download a sample of the 
table into our Python session, fit a model using a Scikit Learn pipeline, and
then use Orbital to translate the steps and estimates into a SQL statement. 
Finally, we will use that SQL statement as the base to compare the current 
interest against the prediction, and download the loans that had a large 
difference. See @fig-diagram.

::: {#fig-diagram}
```{mermaid}
flowchart LR
  A[1-Full Table] --Download--> B(2-Sample) 
  B--Scikit Learn fit-->C(3-Model)
  C--Orbital parse-->D(4-SQL)
  D--Predict-->A
```

Diagram of the approach used for this use case
:::

## Download sample

1.  Load necessary libraries. Make sure to have `databricks-sql-connector`
installed in your environment, that is the source of `databricks`.

    ```{python}
    from dotenv import load_dotenv
    from databricks import sql
    import pandas as pd
    import os
    ```

2.  Load the credentials to be used via their respective environment variables.

    ```{python}
    load_dotenv()
    host = os.getenv("DATABRICKS_HOST")
    token = os.getenv("DATABRICKS_TOKEN")
    ```

3.  For simplicity's sake, the table's catalog, schema and HTTP path into variables.

    ```{python}
    schema = "end-to-end"
    catalog = "sol_eng_demo_nickp"
    http_path = "/sql/1.0/warehouses/b71952ebceb705ce"
    ```

4.  Establish the database connection using the defined variables 

    ```{python}
    con = sql.connect(host, http_path, token, catalog = catalog, schema = schema)
    ```

5.  Using `TABLESAMPLE`, download 100 rows. `REPEATABLE` is used for purposes
of reproducibility.

    ```{python}
    con_cursor = con.cursor()
    con_cursor.execute(
      "select * from loans_full_schema TABLESAMPLE (100 ROWS) REPEATABLE (999);"
      )
    ```

6.  Iterate through the field descriptions to extract their respective names

    ```{python}
    col_names = [desc[0] for desc in con_cursor.description]
    ```

7.  Convert the downloaded data into Pandas

    ```{python}
    res = con_cursor.fetchall()
    full_df = pd.DataFrame(res, columns=col_names)
    ```


## Fit locally

1.  Load the appropiate Scikit Learn modules
    
    ```{python}
    from sklearn.model_selection import train_test_split
    from sklearn.linear_model import LinearRegression
    from sklearn.preprocessing import StandardScaler
    from sklearn.compose import ColumnTransformer
    from sklearn.pipeline import Pipeline
    ```

2.  Select the fields that will be used for predictors, and add them to a list 
called `pred_names`. 

    ```{python}
    pred_names = ["annual_income", "total_credit_lines", "loan_amount",  "term"]
    ```

3.  Subset the data into a new variable (`predictors`)

    ```{python}
    predictors = full_df[pred_names]
    ```
    
4.  Pull the interest rate field from the data into a new variable (`outcome`)
    
    ```{python}
    outcome = full_df["interest_rate"]
    ```

5.  Split the rows into train and test

    ```{python}
    pred_train, pred_test, out_train, out_test = train_test_split(
        predictors, outcome, test_size=20, random_state=999
    )
    ```

6.  Create the pipeline. Use `pre_names` to define the fields to run the scaler
    against
    
    ```{python}
    pipeline = Pipeline(
        [("preprocess", 
          ColumnTransformer(
            [("scaler", StandardScaler(with_std=False), pred_names)],
            remainder="passthrough")
            ),
        ("linear_regression", LinearRegression())]
    )
    ```

7.  Fit the pipeline

    ```{python}
    pipeline.fit(pred_train, out_train)
    ```


## Convert to SQL using Orbital

1.  Import Orbital
    ```{python}
    import orbitalml
    import orbitalml.types
    ```

2.  Parse the pipeline with Orbital. At this stage, you can define the predictor's
field and types, as well as any other fields that need to be included in the
final result set.
    
    ```{python}
    orbital_pipeline = orbitalml.parse_pipeline(
      pipeline, 
      features={
        "annual_income": orbitalml.types.DoubleColumnType(),
        "total_credit_lines": orbitalml.types.DoubleColumnType(),
        "loan_amount": orbitalml.types.DoubleColumnType(),    
        "term": orbitalml.types.DoubleColumnType(),
        "loan_id": orbitalml.types.Int32ColumnType(),
        "emp_title": orbitalml.types.StringColumnType(),
        "loan_amount": orbitalml.types.DoubleColumnType(),
        "balance": orbitalml.types.DoubleColumnType(),
        "application_type": orbitalml.types.StringColumnType(),
        "interest_rate": orbitalml.types.DoubleColumnType()
        })
    ```

2.  Convert the pipeline to SQL. By default, Orbital will exclude the predictor
fields from the finalized SQL statement, so `ResultsProjection()` is used to
force the loan amount and loan term to be included in the statement.

    ```{python}
    pred_sql = orbitalml.export_sql(
        table_name="loans_full_schema", 
        pipeline=orbital_pipeline, 
        projection= orbitalml.ResultsProjection(["loan_amount", "term"]),
        dialect="databricks"
        )
    ```
    
3.  Preview the resulting SQL statment    
    
    ```{python}    
    pred_sql
    ```

## Predict against full table

1.  Use the new SQL statement as the source to filter for any rate that is 15 
points above the prediction

    ```{python}
    final_sql = f"select * from ({pred_sql}) where interest_rate - variable > 15 and variable > 0"
    
    final_sql
    ```

2.  Execute the finalized SQL statement, and return it as a Pandas data frame

    ```{python}
    con_cursor = con.cursor()
    con_cursor.execute(final_sql)
    pred_cols = [desc[0] for desc in con_cursor.description]
    res = con_cursor.fetchall()
    pred_df = pd.DataFrame(res, columns=pred_cols)
    pred_df
    ```

## Appendix

### Data in example

The data used for this example was downloaded from OpenIntro. The page containing
the description of the data and the download link are available here: 
[loans_full_schema](https://www.openintro.org/data/index.php?data=loans_full_schema).
The CSV file was manually [uploaded to the Databricks Unity Catalog](https://docs.databricks.com/aws/en/ingestion/file-upload/upload-to-volume).

The `loan_id` field is not included in the data file. That was created using the
following two SQL commands. This one to create the field:

```sql
ALTER TABLE sol_eng_demo_nickp.`end-to-end`.loans_full_schema 
ADD COLUMN loan_id BIGINT;
```

This is the SQL command to populate the table with a sequential number:

```sql
WITH cte AS (
  SELECT
    loan_id,
    ROW_NUMBER() OVER (ORDER BY debt_to_income) AS new_loan_id
  FROM
    sol_eng_demo_nickp.`end-to-end`.loans_full_schema
)
MERGE INTO sol_eng_demo_nickp.`end-to-end`.loans_full_schema AS target
USING cte
ON target.debt_to_income = cte.debt_to_income
WHEN MATCHED THEN
  UPDATE SET target.loan_id = cte.new_loan_id;
```

### Python environment

The following library requirements were used to run the example:

```bash
dotenv>=0.9.9
onnx==1.17.0
orbitalml>=0.1.0
pandas>=2.2.3
databricks-sql-connector
```

The version for `onnx` was fixed to 1.7.0 because the current version, 
1.8.0, has a [bug](https://github.com/onnx/onnx/issues/6962) that 
prevents `orbital` from working. The bug has been fixed in the development
version, so if you the next release should be fine to use.


### Full code

```{python}
#| eval: false
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from dotenv import load_dotenv
from databricks import sql
import orbitalml.types
import pandas as pd
import orbitalml
import os

load_dotenv()
host = os.getenv("DATABRICKS_HOST")
token = os.getenv("DATABRICKS_TOKEN")
schema = "end-to-end"
catalog = "sol_eng_demo_nickp"
http_path = "/sql/1.0/warehouses/b71952ebceb705ce"
con = sql.connect(host, http_path, token, catalog = catalog, schema = schema)
con_cursor = con.cursor()
con_cursor.execute(
  "select * from loans_full_schema TABLESAMPLE (100 ROWS) REPEATABLE (999);"
  )
col_names = [desc[0] for desc in con_cursor.description]
res = con_cursor.fetchall()
full_df = pd.DataFrame(res, columns=col_names)

pred_names = ["annual_income", "total_credit_lines", "loan_amount",  "term"]
predictors = full_df[pred_names]
outcome = full_df["interest_rate"]
pred_train, pred_test, out_train, out_test = train_test_split(
    predictors, outcome, test_size=20, random_state=999
)
pipeline = Pipeline(
    [("preprocess", 
      ColumnTransformer(
        [("scaler", StandardScaler(with_std=False), pred_names)],
        remainder="passthrough")
        ),
    ("linear_regression", LinearRegression())]
)
pipeline.fit(pred_train, out_train)

orbital_pipeline = orbitalml.parse_pipeline(
  pipeline, 
  features={
    "annual_income": orbitalml.types.DoubleColumnType(),
    "total_credit_lines": orbitalml.types.DoubleColumnType(),
    "loan_amount": orbitalml.types.DoubleColumnType(),    
    "term": orbitalml.types.DoubleColumnType(),
    "loan_id": orbitalml.types.Int32ColumnType(),
    "emp_title": orbitalml.types.StringColumnType(),
    "loan_amount": orbitalml.types.DoubleColumnType(),
    "balance": orbitalml.types.DoubleColumnType(),
    "application_type": orbitalml.types.StringColumnType(),
    "interest_rate": orbitalml.types.DoubleColumnType()
    })
pred_sql = orbitalml.export_sql(
    table_name="loans_full_schema", 
    pipeline=orbital_pipeline, 
    projection= orbitalml.ResultsProjection(["loan_amount", "term"]),
    dialect="databricks"
    )

final_sql = f"select * from ({pred_sql}) where interest_rate - variable > 15 and variable > 0"
con_cursor.execute(final_sql)
pred_cols = [desc[0] for desc in con_cursor.description]
res = con_cursor.fetchall()
pred_df = pd.DataFrame(res, columns=pred_cols)
pred_df
```