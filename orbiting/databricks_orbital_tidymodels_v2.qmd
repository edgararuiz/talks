---
title: Databricks + Tidymodels + Orbital
subtitle: Simple use case example
author: Edgar Ruiz - Posit
date: 2025-05-22
format: gfm
execute:
  eval: true
toc: true  
toc-expand: true
---

```{r}
#| include: false
library(tidymodels)
library(tidyverse)
library(dbplyr)
options("odbc.no_config_override"=FALSE)
```

## Use case

Using loan data, we want to use a model that estimates an appropriate interest rate, and then use that model to find out if the interest rate for a given loan may have been too high. The loan data is in a table located in the Databricks Unity Catalog. The ultimate objective of the project, is to have it check on a daily basis to see what loans may an issue.

## Approach

*"Fit small, predict big"*

To make it as close to a 'real-life' scenario, we will download a sample of the table into our R session, fit a model using a Tidymodels, and then use Orbital to translate the steps and estimates into a SQL statement. Finally, we will use that SQL statement as the base to compare the current interest against the prediction, and download the loans that had a large difference. Thanks to the integrated environment in Databricks, the resulting SQL statement will be saved in the Databricks Workspace, and used to run on a schedule via a [Databricks Job](https://docs.databricks.com/aws/en/jobs/).

::: {#fig-diagram}
```{mermaid}
flowchart LR
  A[1-Full Table] --Download--> B(2-Sample) 
  B--Tidymodels fit-->C(3-Model)
  C--Orbital parse-->D(4-SQL)
  D--Automate-->E(5-Job)
  E--Predict-->A
```

Diagram of the approach used for this use case
:::

## Download sample

1.  The [`odbc::databricks()`](https://odbc.r-dbi.org/reference/databricks.html) function provides lots of convenient features that handle essentially everything needed to establish a connection to Databricks. The only thing to provide is the `httpPath` in order to succeed. The connection is established via `DBI`.

    ```{r}
    library(DBI)

    con <- dbConnect(
      drv = odbc::databricks(), 
      httpPath = "/sql/1.0/warehouses/b71952ebceb705ce"
      )
    ```

2.  Because they will be used in multiple locations in this example, the `catalog`, `schema`, and `table` name are loaded to variables

    ```{r}
    catalog <- "sol_eng_demo_nickp"
    schema <- "end-to-end"
    table <- "loans_full_schema"
    ```

3.  Sample the database table using the `TABLESAMPLE` SQL function. `REPEATABLE` is used to aid with reproducibility. The `glue_sql()` function is used to compile the SQL statement.

    ```{r}
    library(glue)

    sample_sql <- glue_sql(
      "SELECT * ", 
      "FROM ",
      "{`catalog`}.{`schema`}.{`table`}",
      "TABLESAMPLE (100 ROWS) REPEATABLE (999)",
      .con = con
      )

    sample_sql
    ```

4.  The sample is downloaded by executing the SQL statement via `dbGetQuery()`.

    ```{r}
    sample_lending <- dbGetQuery(
      conn = con, 
      statement = sample_sql
      )
    ```

## Fit locally

1.  Load `tidymodels` and set the seed

    ```{r}
    library(tidymodels)
    set.seed(999)
    ```

2.  Currently, **certain fields are downloaded from Databricks as Integer 64 type. These are not supported by R in general**. The easiest solution is to convert them to double. This needs to only be done in the local copy, since goal is to have a SQL statement that will run inside Databricks. To make the data transformation easy, we will use `dplyr`.

    ```{r}
    library(dplyr)

    sample_lending <- sample_lending |> 
      mutate(
        total_credit_lines = as.double(total_credit_lines),
        loan_amount = as.double(loan_amount),
        term = as.double(term)
        )
    ```

3.  Split the data into training and testing.

    ```{r}
    split_lending <- initial_split(sample_lending)

    lending_training <- training(split_lending)
    ```

4.  Create a `recipe` that defines the predictors and outcome fields, and includes a normalization step. For this example, we will use the `annual_income`, `total_credit_lines`, `loan_amount` and `term` fields for predictors.

    ```{r}
    rec_lending <- recipe(
      interest_rate ~ annual_income + total_credit_lines + loan_amount + term, 
      data = lending_training
      ) |> 
      step_normalize(all_numeric_predictors())
    ```

5.  Define a linear regression model spec.

    ```{r}
    lm_spec <- linear_reg()
    ```

6.  Create the workflow by combining the recipe and the defined model spec.

    ```{r}
    wf_spec <- workflow(rec_lending, lm_spec)
    ```

7.  Fit the workflow using the training data, and preview

    ```{r}
    wf_fit <- fit(wf_spec, lending_training)

    wf_fit
    ```

## Convert to SQL using Orbital

1.  Load and use `orbital` to read the fitted workflow. In Databricks, names with dots (".") are not acceptable, and `.pred` is the default name that `orbital` gives the prediction. To fix, use the `prefix` to override.

    ```{r}
    library(orbital)

    lending_orbital <-  orbital(wf_fit, prefix = "pred")

    lending_orbital
    ```

2.  Load `dbplyr`, and use `tbl` to create a reference to the lending table in the R session. To pass the fully qualified name, we can use `glue` and `I()`.

    ```{r}
    library(dbplyr)

    tbl_lending <- tbl(con, I(glue("{catalog}.`{schema}`.{table}")))

    tbl_lending
    ```

3.  In order to make the predictions part of a larger set of fields returned by the final query, the `orbital_inline()` function is used. This allows for it to be passed inside a `dplyr` `mutate()` call. `orbital_inline()` will modify the predictor fields based on the steps from the recipe, which in the example's case, is the normalization step. A quick way to retain the original values, if they are to be part of the final result, is to simply create copies of them via `mutate()`. Finally, since it is not necessary to return all of the fields, we reduce them via a `select()` call.

    ```{r}
    tbl_prep <- tbl_lending |> 
      mutate(o_annual_income = annual_income, 
             o_total_credit_lines = total_credit_lines,
             o_loan_amount = loan_amount, 
             o_term = term
             ) |> 
      mutate(!!! orbital_inline(lending_orbital)) |> 
      select(
        pred, interest_rate, emp_title, balance, application_type,
        o_annual_income, o_total_credit_lines, o_loan_amount, o_term
      )
    ```

4.  Preview the top rows from the initial transformations.

    ```{r}
    tbl_prep |> 
      head()
    ```

5.  An additional step is added to only keep the rows that have an current interest rate is 15 points higher than the prediction.

    ```{r}
    tbl_final <- tbl_prep |> 
      filter(interest_rate - pred > 15, pred > 0)
    ```

6.  Preview the results

    ```{r}
    tbl_final
    ```

7.  Preview the actual SQL that will be sent using `show_query()`.

    ```{r}
    tbl_final |> 
      show_query()
    ```

8.  `show_query()` is mostly geared towards having a nice output to the R console. To capture the SQL in a variable, use `remote_query()`.

    ```{r}
    final_sql <- remote_query(tbl_final)
    ```

9.  As a way to confirm that the SQL will run as returned by `remote_query`, use `dbQuery()` to run the statement against the database.

    ```{r}
    res <- dbGetQuery(con, final_sql)
    head(res)
    ```

## Automate in Databricks

The [`brickster`](https://github.com/databrickslabs/brickster) package provides
an easy-to-use interface with Databricks. It allows us to create, setup and
manage assets within Databricks, such as jobs, files and warehouses.


1.  The SQL tasks require a warehouse ID. `db_sql_warehouse_list()` retrieves
all available warehouses and their details. The warehouse to be used is selected
based on its name
    ```{r}
    library(brickster)
    
    warehouses <- db_sql_warehouse_list()
    
    wh_names <- as.character(lapply(warehouses, function(x) x$name))
    
    warehouse <- unlist(warehouses[wh_names == "David's Serverless Warehouse"])
    
    warehouse_id <- warehouse[["id"]]
    ```

2.  The first step is to create a named query in Databricks. This is where  
`final_sql` is used
    ```{r}
    db_request_query <- db_query_create(
      query_text = final_sql,
      catalog = catalog,
      schema = schema,
      display_name = "Interest rate differences",
      warehouse_id = warehouse_id,
      description = "Find differences in interest rate"
    )
    ```


3.  A Databricks Job can be used to orchestrate multiple tasks. The job being 
created for this example requires a single task. Tasks could be a variety of
kinds such as a Notebook, Python script, SQL Queries and many others. The 
following starts building the SQL task. It uses the `db_request_query`’s ID to
tie it to the query created in the previous step.

    ```{r}
    db_task_query <- sql_query_task(
      query_id = db_request_query$id, 
      warehouse_id = warehouse_id
    )
    ```

4.  The SQL task object is used to create a formal Task. This is the point where
the resulting task in the Databricks UI can be named and described
    ```{r}
    db_task_job <- job_task(
      task = db_task_query, 
      description = "Int rate diffs", 
      task_key = "run_sql"
      )
    ```

5.  For this example, the Job needs to have a schedule defined. And to do this,
a CronSchedule object is needed. To define the schedule use a 
[CRON expression](https://www.quartz-scheduler.org/documentation/)

    ```{r}
    db_schedule <- cron_schedule(
      quartz_cron_expression = "0 0 12 * * ?", 
      timezone_id = "CST"
      )
    ```

6.  Finally, the Job is created using the task and schedule objects. After 
running the following command, a new Job named “Daily check interest 
differences” will appear in the Jobs section of the Databricks Web UI.
    ```{r}
    new_job <- db_jobs_create(
      name = "Daily check interest differences",
      tasks = job_tasks(db_task_job),    
      schedule = db_schedule
    )
    ```



## Appendix

### Data in example

The data used for this example was downloaded from OpenIntro. The page containing the description of the data and the download link are available here: [loans_full_schema](https://www.openintro.org/data/index.php?data=loans_full_schema). The CSV file was manually [uploaded to the Databricks Unity Catalog](https://docs.databricks.com/aws/en/ingestion/file-upload/upload-to-volume).

The `loan_id` field is not included in the data file. That was created using the following two SQL commands. This one to create the field:

``` sql
ALTER TABLE sol_eng_demo_nickp.`end-to-end`.loans_full_schema 
ADD COLUMN loan_id BIGINT;
```

This is the SQL command to populate the table with a sequential number:

``` sql
WITH cte AS (
  SELECT
    loan_id,
    ROW_NUMBER() OVER (ORDER BY debt_to_income) AS new_loan_id
  FROM
    sol_eng_demo_nickp.`end-to-end`.loans_full_schema
)
MERGE INTO sol_eng_demo_nickp.`end-to-end`.loans_full_schema AS target
USING cte
ON target.debt_to_income = cte.debt_to_income
WHEN MATCHED THEN
  UPDATE SET target.loan_id = cte.new_loan_id;
```
