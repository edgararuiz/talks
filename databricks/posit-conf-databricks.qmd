---
format: 
  revealjs:
    width: 1250
    height: 740
    max-scale: 1
    min-scale: 1
    smaller: true
    transition: fade
    background-transition: fade
    theme: [default, theme.scss]
    code-line-numbers: false
    menu: false
execute: 
  echo: false
---

# Databricks with R {background-image="assets/title-slide-white.png" background-size="1500px"}

Edgar Ruiz \@ Posit

[linkedin.com/in/edgararuiz](https://www.linkedin.com/in/edgararuiz/)

## Spark Connect {background-image="assets/slide-frame.png" background-size="1500px"}

-   Introduced a decoupled client-server architecture

-   It enables remote connectivity to Spark clusters.

-   **Allows R users to interact with a cluster** from their preferred environment, laptop or otherwise.

-   [Databricks Connect](https://docs.databricks.com/dev-tools/databricks-connect.html), is based on **Spark Connect** architecture. Available in DBR version 13+

```{mermaid}
%%| fig-height: '500px'

flowchart LR
  lp[User's machine]
  sp[Spark]
  
  lp <-. Network .-> sp
  
  style lp  fill:#fff,stroke:#666
  style sp  fill:#f4c430,stroke:#666
```

## Spark Connect {background-image="assets/slide-frame.png" background-size="1500px"}

-   Uses of a *remote procedure call* framework, named **gRPC**.

-   Uses **Torch** for the ML capabilities (Spark 3.5+).

-   `PySpark` offers the best integration with Connect.

```{mermaid}
flowchart LR
  subgraph lp[User's machine]
    ps[PySpark]
    g1[gRPC]
  end
  sp[Spark]
  
  g1 <-. Network .-> sp
  ps --> g1
  
  style ps  fill:#eff,stroke:#666
  style lp  fill:#fff,stroke:#666
  style sp  fill:#f4c430,stroke:#666
  style g1  fill:#447099,stroke:#666,color:#fff
```

## Integrating R via *sparklyr* {background-image="assets/slide-frame.png" background-size="1500px"}

-   Integrates with PySpark, via `reticulate`

-   Extends the functionality, and user experience:

    -   `dplyr` back-end
    -   `DBI` back-end
    -   RStudio's *Connections pane* integration.

```{mermaid}
flowchart LR
  subgraph lp[User's machine]
    sr[sparklyr]
    rt[reticulate]
    ps[PySpark]
    g1[gRPC]
  end
  sp[Spark]
  
  sr --> rt
  rt --> ps
  g1 <-. Network .-> sp
  ps --> g1
  
  style sr  fill:#d0efb1,stroke:#666
  style rt  fill:#d0efb1,stroke:#666
  style ps  fill:#eff,stroke:#666
  style lp  fill:#fff,stroke:#666
  style sp  fill:#f4c430,stroke:#666
  style g1  fill:#447099,stroke:#666,color:#fff
```

:::{.incremental}
- ----------------- No need to install Java in my machine!!! ðŸŽ‰  ----------------- 
:::

## Databricks Connect (DBR 13+) {background-image="assets/slide-frame.png" background-size="1500px"}

`sparklyr` 1.8.3 now supports Databricks Connect "v2":

::: sample-large
``` r
library(sparklyr)

sc <- spark_connect(
  master     = "", # Your org's address
  cluster_id = "", # Your cluster's ID
  token      = "", # Your personal token
  method     = "databricks_connect"
 )
```
:::

## Databricks Connect (DBR 13+) {background-image="assets/slide-frame.png" background-size="1500px"}

`sparklyr` uses environment variables, if provided:

-   `DATABRICKS_HOST` - Your org's address

-   `DATABRICKS_CLUSTER_ID` - Your cluster's ID

-   `DATABRICKS_TOKEN` - Your personal token

Simplifies, and secures connection code:

::: sample-large
``` r
sc <- spark_connect(
  method = "databricks_connect"
 )
```
:::

## Databricks Catalog Explorer {background-image="assets/slide-frame.png" background-size="1500px"}

![](assets/catalog-explorer.png){height="500"}

## Connection Pane {background-image="assets/slide-frame.png" background-size="1500px"}

![](assets/rstudio-connection.png){height="500"}

## Preview top 1000 rows {background-image="assets/slide-frame.png" background-size="1500px"}

![](assets/preview.png){height="500"}

## Accessing the Catalog data {background-image="assets/slide-frame.png" background-size="1500px"}

::: sample-small
```r
library(dplyr)
library(dbplyr)

trips <- tbl(sc, in_catalog("samples", "nyctaxi", "trips"))
```
:::

::: {.fragment .fade-in}
::: sample-small
```r
trips
#> # Source: spark<`samples`.`nyctaxi`.`trips`> [?? x 6]
#>    tpep_pickup_datetime tpep_dropoff_datetime trip_distance fare_amount
#>    <dttm>               <dttm>                        <dbl>       <dbl>
#>  1 2016-02-14 10:52:13  2016-02-14 11:16:04            4.94        19  
#>  2 2016-02-04 12:44:19  2016-02-04 12:46:00            0.28         3.5
#>  3 2016-02-17 11:13:57  2016-02-17 11:17:55            0.7          5  
#>  4 2016-02-18 04:36:07  2016-02-18 04:41:45            0.8          6  
#>  5 2016-02-22 08:14:41  2016-02-22 08:31:52            4.51        17  
#>  6 2016-02-05 00:45:02  2016-02-05 00:50:26            1.8          7  
#>  7 2016-02-15 09:03:28  2016-02-15 09:18:45            2.58        12  
#>  8 2016-02-25 13:09:26  2016-02-25 13:24:50            1.4         11  
#>  9 2016-02-13 10:28:18  2016-02-13 10:36:36            1.21         7.5
#> 10 2016-02-13 18:03:48  2016-02-13 18:10:24            0.6          6  
#> # â„¹ more rows
#> # â„¹ 2 more variables: pickup_zip <int>, dropoff_zip <int>
```
:::
:::


## Interacting with the data {background-image="assets/slide-frame.png" background-size="1500px"}

::: sample-small
```r
trips %>%
  group_by(pickup_zip) %>%
  summarise(
    count = n(),
    avg_distance = mean(trip_distance, na.rm = TRUE)
  )
```
:::

::: {.fragment .fade-in}
::: sample-small
```r
#> # Source: spark<?> [?? x 3]
#>    pickup_zip count avg_distance
#>         <int> <dbl>        <dbl>
#>  1      10032    15         4.49
#>  2      10013   273         2.98
#>  3      10022   519         2.00
#>  4      10162   414         2.19
#>  5      10018  1012         2.60
#>  6      11106    39         2.03
#>  7      10011  1129         2.29
#>  8      11103    16         2.75
#>  9      11237    15         3.31
#> 10      11422   429        15.5 
#> # â„¹ more rows
```
:::
:::

## Supported vs Not {background-image="assets/slide-frame.png" background-size="1500px"}

:::{.columns}
:::{.column width='60%'}

### Supported

- Most of the `dplyr`, and `DBI`, APIs

- `invoke()` command

- Connections Pane navigation

- PAT for Databricks Connect

- Most read and write commands

:::
:::{.column width='40%'}

### Not supported

- ML functions 

- `SDF` functions

- `tidyr` 

:::
:::

## Run Spark Connect locally {background-image="assets/slide-frame.png" background-size="1500px"}

- Install Spark 3.4+ (one time)

::: sample-small
``` r
spark_install("3.4")
```
:::

::: {.fragment .fade-in}
- Start the Spark Connect service using: 

::: sample-small
``` r
pysparklyr::spark_connect_service_start()

#> Starting Spark Connect locally ...
#>   starting org.apache.spark.sql.connect.service.SparkConnectServer, logging to
#>   /Users/edgar/spark/spark-3.4.0-bin-hadoop3/logs/spark-edgar-org.apache.spark.sql.connect.service.SparkConnectServer-1-Edgars-work-laptop.local.out
```
:::
:::

::: {.fragment .fade-in}
- To stop the service use:

::: sample-small
```r
pysparklyr::spark_connect_service_stop()

#> Stopping Spark Connect
#>   - Shutdown command sent
```
:::
:::
 
## Python session {background-image="assets/slide-frame.png" background-size="1500px"}

Easy to interact with the underlying Python components

::: sample-large
```r
library(sparklyr)
sc <- spark_connect("sc://localhost", method = "spark_connect")
```
:::{.fragment .fade-in}
```r

session <- sc$python_obj
```
:::

:::{.fragment .fade-in}
```r

tbl_mtcars <- session$createDataFrame(mtcars)
```
:::

:::{.fragment .fade-in}
```r

tbl_mtcars
#> DataFrame[mpg: double, cyl: double, disp: double, hp: double, drat: double, wt: double, qsec: double, vs: double, am: double, gear: double, carb: double]
```
:::

:::{.fragment .fade-in}
```r

tbl_mtcars$corr("wt", "mpg")
#> [1] -0.8676594
```
:::
::: 

## Going further {background-image="assets/slide-frame.png" background-size="1500px"}

Use `reticulate` to extend functionality

::: sample-medium
```r
library(reticulate)

# Load the Python libraries
pyspark <- import("pyspark")
connect_classification <- import("pyspark.ml.connect.classification")
```

:::{.fragment .fade-in}
```r
# Build an array column that contains the feature variables
feature_fields <- pyspark$sql$functions$array("mpg", "wt")
```
:::
:::{.fragment .fade-in}
```r
tbl_features <- tbl_mtcars$withColumn("features", feature_fields)
```
:::

:::{.fragment .fade-in}
```r
# Instanciate a new model, and designate the label column
log_reg <- connect_classification$LogisticRegression()
```
:::
:::{.fragment .fade-in}
```r
log_reg_label <- log_reg$setLabelCol("am")
```
:::

:::{.fragment .fade-in}
```r
# Fit the model 
model <- log_reg_label$fit(tbl_features)
```
:::
:::{.fragment .fade-in}
```r
model
#> LogisticRegression_144975e223ce
```
:::
:::

## Going further {background-image="assets/slide-frame.png" background-size="1500px"}

Use `reticulate` to extend functionality

::: sample-medium
```r
# Create the predictions
preds <- model$transform(tbl_features)
```
:::{.fragment .fade-in}
```r
# Import data into R 
r_preds <- preds$toPandas()
```
:::
:::{.fragment .fade-in}
```r
head(r_preds)
#>    mpg cyl disp  hp drat    wt  qsec vs am gear carb      features prediction          probability
#> 1 21.0   6  160 110 3.90 2.620 16.46  0  1    4    4   21.00, 2.62          1 0.4157636, 0.5842364
#> 2 21.0   6  160 110 3.90 2.875 17.02  0  1    4    4 21.000, 2.875          1 0.4890891, 0.5109109
#> 3 22.8   4  108  93 3.85 2.320 18.61  1  1    4    1   22.80, 2.32          1 0.2807752, 0.7192248
#> 4 21.4   6  258 110 3.08 3.215 19.44  1  0    3    1 21.400, 3.215          0 0.5734229, 0.4265772
#> 5 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2   18.70, 3.44          0 0.7180533, 0.2819467
#> 6 18.1   6  225 105 2.76 3.460 20.22  1  0    3    1   18.10, 3.46          0 0.7392238, 0.2607762
```
:::
:::

## {background-image="assets/thank-you.png" background-size="1500px"}
