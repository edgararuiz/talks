---
format: 
  revealjs:
    width: 1200
    smaller: true
    transition: fade
    background-transition: fade
    theme: [default, theme.scss]
    menu: false
execute: 
  echo: false
---

# Databricks with R {background-image="assets/title-slide-white.png"}

Edgar Ruiz \@ Posit

[linkedin.com/in/edgararuiz](https://www.linkedin.com/in/edgararuiz/)

## Spark Connect {background-image="assets/slide-frame.png"}

-   Introduced a decoupled client-server architecture

-   It enables remote connectivity to Spark clusters.

-   **Allows R users to interact with a cluster** from their preferred environment, laptop or otherwise.

-   [Databricks Connect](https://docs.databricks.com/dev-tools/databricks-connect.html), is based on **Spark Connect** architecture. Available in DBR version 13+

```{mermaid}
%%| fig-height: '500px'

flowchart LR
  lp[User's machine]
  sp[Spark]
  
  lp <-. Network .-> sp
  
  style lp  fill:#fff,stroke:#666
  style sp  fill:#f4c430,stroke:#666
```

## Spark Connect {background-image="assets/slide-frame.png"}

-   Uses of a *remote procedure call* framework, named **gRPC**.

-   Uses **Torch** for the ML capabilities (Spark 3.5+).

-   `PySpark` offers the best integration with Connect.

```{mermaid}
flowchart LR
  subgraph lp[User's machine]
    ps[PySpark]
    g1[gRPC]
  end
  sp[Spark]
  
  g1 <-. Network .-> sp
  ps --> g1
  
  style ps  fill:#eff,stroke:#666
  style lp  fill:#fff,stroke:#666
  style sp  fill:#f4c430,stroke:#666
  style g1  fill:#447099,stroke:#666,color:#fff
```

## Integrating R via *sparklyr* {background-image="assets/slide-frame.png"}

-   Integrates with PySpark, via `reticulate`

-   Extends the functionality, and user experience:

    -   `dplyr` back-end
    -   `DBI` back-end
    -   RStudio's *Connections pane* integration.

```{mermaid}
flowchart LR
  subgraph lp[User's machine]
    sr[sparklyr]
    rt[reticulate]
    ps[PySpark]
    g1[gRPC]
  end
  sp[Spark]
  
  sr --> rt
  rt --> ps
  g1 <-. Network .-> sp
  ps --> g1
  
  style sr  fill:#d0efb1,stroke:#666
  style rt  fill:#d0efb1,stroke:#666
  style ps  fill:#eff,stroke:#666
  style lp  fill:#fff,stroke:#666
  style sp  fill:#f4c430,stroke:#666
  style g1  fill:#447099,stroke:#666,color:#fff
```

:::{.incremental}
- ----------------- No need to install Java in my machine!!! ðŸŽ‰  ----------------- 
:::

## Databricks Catalog Explorer {background-image="assets/slide-frame.png"}

![](assets/catalog-explorer.png){height="500"}

## Connection Pane {background-image="assets/slide-frame.png"}

![](assets/rstudio-connection.png){height="500"}

## Preview top 1000 rows {background-image="assets/slide-frame.png"}

![](assets/preview.png){height="500"}

## Databricks Connect (DBR 13+) {background-image="assets/slide-frame.png"}

`sparklyr` 1.8.3 now supports Databricks Connect "v2":

::: sample-large
``` r
library(sparklyr)

sc <- spark_connect(
  master     = "", # Your org's address
  cluster_id = "", # Your cluster's ID
  token      = "", # Your personal token
  method     = "databricks_connect"
 )
```
:::

## Databricks Connect (DBR 13+) {background-image="assets/slide-frame.png"}

`sparklyr` uses environment variables, if provided:

-   `DATABRICKS_HOST` - Your org's address

-   `DATABRICKS_CLUSTER_ID` - Your cluster's ID

-   `DATABRICKS_TOKEN` - Your personal token

Simplifies, and secures connection code:

::: sample-large
``` r
sc <- spark_connect(
  method = "databricks_connect"
 )
```
:::

## Accessing the Catalog data {background-image="assets/slide-frame.png"}

::: sample-small
```r
library(dplyr)
library(dbplyr)

trips <- tbl(sc, in_catalog("samples", "nyctaxi", "trips"))

trips
#> # Source: spark<`samples`.`nyctaxi`.`trips`> [?? x 6]
#>    tpep_pickup_datetime tpep_dropoff_datetime trip_distance fare_amount
#>    <dttm>               <dttm>                        <dbl>       <dbl>
#>  1 2016-02-14 10:52:13  2016-02-14 11:16:04            4.94        19  
#>  2 2016-02-04 12:44:19  2016-02-04 12:46:00            0.28         3.5
#>  3 2016-02-17 11:13:57  2016-02-17 11:17:55            0.7          5  
#>  4 2016-02-18 04:36:07  2016-02-18 04:41:45            0.8          6  
#>  5 2016-02-22 08:14:41  2016-02-22 08:31:52            4.51        17  
#>  6 2016-02-05 00:45:02  2016-02-05 00:50:26            1.8          7  
#>  7 2016-02-15 09:03:28  2016-02-15 09:18:45            2.58        12  
#>  8 2016-02-25 13:09:26  2016-02-25 13:24:50            1.4         11  
#>  9 2016-02-13 10:28:18  2016-02-13 10:36:36            1.21         7.5
#> 10 2016-02-13 18:03:48  2016-02-13 18:10:24            0.6          6  
#> # â„¹ more rows
#> # â„¹ 2 more variables: pickup_zip <int>, dropoff_zip <int>
```
:::


## Accessing the Catalog data {background-image="assets/slide-frame.png"}

::: sample-small
```r
trips %>%
  group_by(pickup_zip) %>%
  summarise(
    count = n(),
    avg_distance = mean(trip_distance, na.rm = TRUE)
  )
#> # Source: spark<?> [?? x 3]
#>    pickup_zip count avg_distance
#>         <int> <dbl>        <dbl>
#>  1      10032    15         4.49
#>  2      10013   273         2.98
#>  3      10022   519         2.00
#>  4      10162   414         2.19
#>  5      10018  1012         2.60
#>  6      11106    39         2.03
#>  7      10011  1129         2.29
#>  8      11103    16         2.75
#>  9      11237    15         3.31
#> 10      11422   429        15.5 
#> # â„¹ more rows
```
:::

## Supported vs Not {background-image="assets/slide-frame.png"}

:::{.columns}
:::{.column width='60%'}

### Supported

- Most of the `dplyr`, and `DBI`, APIs

- `invoke()` command

- Connections Pane navigation

- PAT for Databricks Connect

- Most read and write commands

:::
:::{.column width='40%'}

### Not supported

- ML functions 

- `SDF` functions

- `tidyr` 

:::
:::

## Spark Connect, locally {background-image="assets/slide-frame.png"}

- Install Spark 3.4+ (one time)

::: sample-small
``` r
spark_install("3.4")
```
:::

- Start the Spark Connect service using: 

::: sample-small
``` r
pysparklyr::spark_connect_service_start()

#> Starting Spark Connect locally ...
#>   starting org.apache.spark.sql.connect.service.SparkConnectServer, logging to
#>   /Users/edgar/spark/spark-3.4.0-bin-hadoop3/logs/spark-edgar-org.apache.spark.sql.connect.service.SparkConnectServer-1-Edgars-work-laptop.local.out
```
:::

- To stop the service use:

::: sample-small
```r
pysparklyr::spark_connect_service_stop()

#> Stopping Spark Connect
#>   - Shutdown command sent
```
:::


## {background-image="assets/thank-you.png"}
