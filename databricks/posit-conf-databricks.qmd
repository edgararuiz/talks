---
format: 
  revealjs:
    width: 1255
    height: 740
    max-scale: 1
    min-scale: 1
    smaller: true
    transition: fade
    background-transition: fade
    theme: [default, theme.scss]
    code-line-numbers: false
    menu: false
execute: 
  echo: false
engine: knitr
---

# Databricks with R {background-image="assets/title-slide-white.png" background-size="1500px" background-color="white"}

Edgar Ruiz \@ Posit

[linkedin.com/in/edgararuiz](https://www.linkedin.com/in/edgararuiz/)

# Spark Connect{background-image="assets/slide-frame-dark.png" background-size="1500px" background-color="#2a7070"}

:::{.dark-slide}
'Thin client, with the full power of Apache Spark'
:::

## Spark Connect {background-image="assets/slide-frame.png" background-size="1500px" background-color="white"}

-   Introduced a decoupled client-server architecture

-   It enables remote connectivity to Spark clusters.

-   **Allows R users to interact with a cluster** from their preferred environment, laptop or otherwise.

-   [Databricks Connect](https://docs.databricks.com/dev-tools/databricks-connect.html), is based on **Spark Connect** architecture. Available in DBR version 13+

```{mermaid}
%%| fig-height: '500px'

flowchart LR
  lp[User's machine]
  sp[Spark]
  
  lp <-. Network .-> sp
  
  style lp  fill:#fff,stroke:#666
  style sp  fill:#f4c430,stroke:#666
```

## Underlying technologies {background-image="assets/slide-frame.png" background-size="1500px" background-color="white"}

-   Uses of a *remote procedure call* framework, named **gRPC**.

-   Uses **Torch** for the ML capabilities (Spark 3.5+).

-   `PySpark` offers the best integration with Spark Connect.

```{mermaid}
flowchart LR
  subgraph lp[User's machine]
    ps[PySpark]
    g1[gRPC]
  end
  sp[Spark]
  
  g1 <-. Network .-> sp
  ps --> g1
  
  style ps  fill:#eff,stroke:#666
  style lp  fill:#fff,stroke:#666
  style sp  fill:#f4c430,stroke:#666
  style g1  fill:#447099,stroke:#666,color:#fff
```

## Integrating R, via *sparklyr* {background-image="assets/slide-frame.png" background-size="1500px" background-color="white"}

-   Integrates with PySpark, via `reticulate`

-   Extends the functionality, and user experience:

    -   `dplyr` back-end
    -   `DBI` back-end
    -   RStudio's *Connections pane* integration.

```{mermaid}
flowchart LR
  subgraph lp[User's machine]
    sr[sparklyr]
    rt[reticulate]
    ps[PySpark]
    g1[gRPC]
  end
  sp[Spark]
  
  sr --> rt
  rt --> ps
  g1 <-. Network .-> sp
  ps --> g1
  
  style sr  fill:#d0efb1,stroke:#666
  style rt  fill:#d0efb1,stroke:#666
  style ps  fill:#eff,stroke:#666
  style lp  fill:#fff,stroke:#666
  style sp  fill:#f4c430,stroke:#666
  style g1  fill:#447099,stroke:#666,color:#fff
```

:::{.fragment}
-**---------------------- No need to install Java in my machine!!! ðŸŽ‰  ----------------------**
:::

## Slight catch {background-image="assets/slide-frame.png" background-size="1500px" background-color="white"}

Along with **Python 3.9** or above, you will need an environment with the following Python libraries installed:

:::{.columns}
:::{.column width="40%"}
- `pyspark`
- `pandas`
- `PyArrow`
- `grpcio`
- `grpcio_status`
:::
:::{.column width="60%"}
- `google-api-python-client`
- `databricks-connect` *(if using Databricks)*
- `delta-spark`
- `torch` *(Spark 3.5+)*
- `torcheval` *(Spark 3.5+)*
:::
:::

## Getting started is easy {background-image="assets/slide-frame.png" background-size="1500px" background-color="white"}

The new functionality is available in an `sparklyr` extension called `pysparklyr`.
It contains a function that creates and prepares your Python environment.

:::{.sample-medium}

:::{.fragment}
```r
install.packages("sparklyr")
remotes::install_github("mlverse/pysparklyr")
```
:::

:::{.fragment}
```r
pysparklyr::install_pyspark()
```
:::
:::{.fragment}

```r
#> Using Python: /Users/edgar/.pyenv/versions/3.10.13/bin/python3.10
#> Creating virtual environment 'r-sparklyr' ... 
#> + /Users/edgar/.pyenv/versions/3.10.13/bin/python3.10 -m venv /Users/edgar/.virtualenvs/r-sparklyr
#> Done!
#> Installing packages: pip, wheel, setuptools
#> + /Users/edgar/.virtualenvs/r-sparklyr/bin/python -m pip install --upgrade --no-user pip wheel setuptools
#> Collecting pip
#>   Using cached pip-23.2.1-py3-none-any.whl (2.1 MB)
...
#> Successfully installed MarkupSafe-2.1.3 PyArrow-13.0.0 cachetools-5.3.1 certifi-2023.7.22 charset-normalizer-3.2.0 databricks-connect-13.3.0 databricks-sdk-0.8.0 delta-spark-2.4.0 filelock-3.12.4 google-api-core-2.11.1 google-api-python-client-2.99.0 google-auth-2.23.0 google-auth-httplib2-0.1.1 googleapis-common-protos-1.60.0 grpcio-1.58.0 grpcio_status-1.58.0 httplib2-0.22.0 idna-3.4 importlib-metadata-6.8.0 jinja2-3.1.2 mpmath-1.3.0 networkx-3.1 numpy-1.25.2 pandas-2.1.0 protobuf-4.24.3 py4j-0.10.9.7 pyasn1-0.5.0 pyasn1-modules-0.3.0 pyparsing-3.1.1 pyspark-3.4.1 python-dateutil-2.8.2 pytz-2023.3.post1 requests-2.31.0 rsa-4.9 six-1.16.0 sympy-1.12 torch-2.0.1 torcheval-0.0.7 typing-extensions-4.7.1 tzdata-2023.3 uritemplate-4.1.1 urllib3-1.26.16 zipp-3.16.2
```
:::
:::

## Run Spark Connect locally {background-image="assets/slide-frame.png" background-size="1500px" background-color="white"}

- Install Spark 3.4+ (one time)

::: sample-medium
``` r
spark_install("3.4")
```
:::

::: {.fragment .fade-in}
- Start the Spark Connect service using: 

::: sample-medium
``` r
pysparklyr::spark_connect_service_start()

#> Starting Spark Connect locally ...
#>   starting org.apache.spark.sql.connect.service.SparkConnectServer, logging to
#>   /Users/edgar/spark/spark-3.4.0-bin-hadoop3/logs/spark-edgar-org.apache.spark.sql.connect.service.SparkConnectServer-1-Edgars-work-laptop.local.out
```
:::
:::

::: {.fragment .fade-in}
- To stop the service use:

::: sample-medium
```r
pysparklyr::spark_connect_service_stop()

#> Stopping Spark Connect
#>   - Shutdown command sent
```
:::
:::

# Databricks Connect{background-image="assets/slide-frame-dark.png" background-size="1500px" background-color="#2a7070"}

:::{.dark-slide}
'Use from any application, running anywhere'
:::

## Databricks Connect "v2" {background-image="assets/slide-frame.png" background-size="1500px" background-color="white"}

::: sample-large
- Based on Spark Connect 

- Available on DBR 13 onward

- Databricks Connect becomes a thin client that is simple and easy to use

- `sparklyr` 1.8.3, along with `pysparklyr`, supports Databricks Connect:

    ``` r
    library(sparklyr)
    
    sc <- spark_connect(
      master     = "", # Your org's address
      cluster_id = "", # Your cluster's ID
      token      = "", # Your personal token
      method     = "databricks_connect"
     )
    ```
:::

## Databricks Connect {background-image="assets/slide-frame.png" background-size="1500px" background-color="white"}

`sparklyr` uses environment variables, if provided:

-   `DATABRICKS_HOST` - Your org's address

-   `DATABRICKS_TOKEN` - Your personal token

Simplifies, and secures connection code:

::: sample-large
``` r
sc <- spark_connect(
  cluster_id = "{your cluster id}", 
  method = "databricks_connect"
 )
```
:::

## Explore data in the Unity Catalog {background-image="assets/slide-frame.png" background-size="1500px" background-color="white" auto-animate=true}

![](assets/catalog-explorer.png){.absolute top=80 left=50 width=1100}


## Now inside RStudio... {background-image="assets/slide-frame.png" background-size="1500px" background-color="white" auto-animate=true}

![](assets/catalog-explorer.png){.absolute top=200 left=1000 width=1}
![](assets/rstudio-full.png){.absolute top=60 left=30}
![](assets/rstudio-connections-pane.png){.absolute top=84 left=718 width=500}


## Connections pane! {background-image="assets/slide-frame.png" background-size="1500px" background-color="white" auto-animate=true}

![](assets/rstudio-connections-pane.png){.absolute top=80 left=220 width=800}

## Matches the structure in UC {background-image="assets/slide-frame.png" background-size="1500px" background-color="white" auto-animate=true}

![](assets/catalog-explorer-closeup.png){.absolute top=120 left=0}

![](assets/rstudio-connections-pane.png){.absolute top=120 left=650 width=600}

## And of course... {background-image="assets/slide-frame.png" background-size="1500px" background-color="white" auto-animate=true}

![](assets/catalog-explorer-closeup.png){.absolute top=120 left=0}

![](assets/rstudio-connections-pane.png){.absolute top=120 left=0 width=600}

![](assets/preview.png){.absolute top=300 left=560 width=1}

## preview the top 1K rows{background-image="assets/slide-frame.png" background-size="1500px" background-color="white" auto-animate=true}

![](assets/rstudio-connections-pane.png){.absolute top=120 left=0 width=550}

![](assets/preview.png){.absolute top=125 left=560 width=750}


## Accessing the Catalog data {background-image="assets/slide-frame.png" background-size="1500px" background-color="white"}

::: sample-medium
```r
library(dplyr)
library(dbplyr)

trips <- tbl(sc, in_catalog("samples", "nyctaxi", "trips"))
```
:::

::: {.fragment .fade-in}
::: sample-medium
```r
trips
#> # Source: spark<`samples`.`nyctaxi`.`trips`> [?? x 6]
#>    tpep_pickup_datetime tpep_dropoff_datetime trip_distance fare_amount
#>    <dttm>               <dttm>                        <dbl>       <dbl>
#>  1 2016-02-14 10:52:13  2016-02-14 11:16:04            4.94        19  
#>  2 2016-02-04 12:44:19  2016-02-04 12:46:00            0.28         3.5
#>  3 2016-02-17 11:13:57  2016-02-17 11:17:55            0.7          5  
#>  4 2016-02-18 04:36:07  2016-02-18 04:41:45            0.8          6  
#>  5 2016-02-22 08:14:41  2016-02-22 08:31:52            4.51        17  
#>  6 2016-02-05 00:45:02  2016-02-05 00:50:26            1.8          7  
#>  7 2016-02-15 09:03:28  2016-02-15 09:18:45            2.58        12  
#>  8 2016-02-25 13:09:26  2016-02-25 13:24:50            1.4         11  
#>  9 2016-02-13 10:28:18  2016-02-13 10:36:36            1.21         7.5
#> 10 2016-02-13 18:03:48  2016-02-13 18:10:24            0.6          6  
#> # â„¹ more rows
#> # â„¹ 2 more variables: pickup_zip <int>, dropoff_zip <int>
```
:::
:::

## Interacting with the data {background-image="assets/slide-frame.png" background-size="1500px" background-color="white"}

::: sample-medium
```r
trips %>%
  group_by(pickup_zip) %>%
  summarise(
    count = n(),
    avg_distance = mean(trip_distance, na.rm = TRUE)
  )
```
:::

::: {.fragment .fade-in}
::: sample-medium
```r
#> # Source: spark<?> [?? x 3]
#>    pickup_zip count avg_distance
#>         <int> <dbl>        <dbl>
#>  1      10032    15         4.49
#>  2      10013   273         2.98
#>  3      10022   519         2.00
#>  4      10162   414         2.19
#>  5      10018  1012         2.60
#>  6      11106    39         2.03
#>  7      10011  1129         2.29
#>  8      11103    16         2.75
#>  9      11237    15         3.31
#> 10      11422   429        15.5 
#> # â„¹ more rows
```
:::
:::

## {background-image="assets/slide-frame.png" background-size="1500px" background-color="white"}

<br>
<br>
<br>
<br>
<br>
<br>

### And, coming soon...

## Databricks cluster management {background-image="assets/slide-frame.png" background-size="1500px" background-color="white" auto-animate=true}

![](assets/wb-compute.png){.absolute top=80 left=50 width=1100}

## Inside RStudio! ðŸŽ‰ðŸŽ‰ {background-image="assets/slide-frame.png" background-size="1500px" background-color="white" auto-animate=true}

![](assets/wb-compute.png){.absolute top=400 left=1000 width=10}
![](assets/wb-rstudio.png){.absolute top=80 left=50 width=1100}

![](assets/wb-pane.png){.absolute top=295 left=690 width=459}


## Inside RStudio! ðŸŽ‰ðŸŽ‰ {background-image="assets/slide-frame.png" background-size="1500px" background-color="white" auto-animate=true}

![](assets/wb-pane.png){.absolute top=60 left=270 width=750}

## Expand to view details {background-image="assets/slide-frame.png" background-size="1500px" background-color="white" auto-animate=true}

![](assets/wb-pane-expanded.png){.absolute top=60 left=270 width=750}

## Side-by-side {background-image="assets/slide-frame.png" background-size="1500px" background-color="white" auto-animate=true}

![](assets/wb-pane-expanded.png){.absolute top=130 left=800 width=500}
![](assets/wb-cluster.png){.absolute top=130 left=0 width=790}

# Additional info{background-image="assets/slide-frame-dark.png" background-size="1500px" background-color="#2a7070"}

:::{.dark-slide}
What is supported, and some advanced functionality
:::

## Spark Connect & Databricks Connect {background-image="assets/slide-frame.png" background-size="1500px" background-color="white"}

<br>

:::{.columns}
:::{.column width='60%'}

### Supported

- Most of the `dplyr`, and `DBI`, APIs

- `invoke()` command

- Connections Pane navigation

- PAT for Databricks Connect

- Most read and write commands

:::

:::{.column width='40%'}


### Not supported

- ML functions 

- `SDF` functions

- `tidyr` 

:::
:::

## Access the entire API {background-image="assets/slide-frame.png" background-size="1500px" background-color="white"}

Easy to interact with the underlying Python components

::: sample-large
```r
sc <- spark_connect("sc://localhost", method = "spark_connect")
```
:::{.fragment .fade-in}
```r

# Point to the `reticulate` Python object
session <- sc$python_obj
```
:::

:::{.fragment .fade-in}
```r

# Copies the data from R into Spark and returns a PySpark dataframe
tbl_mtcars <- session$createDataFrame(mtcars)
```
:::

:::{.fragment .fade-in}
```r

# Object doesn't print the data, prints the PySpark dataframe's metadata
tbl_mtcars
#> DataFrame[mpg: double, cyl: double, disp: double, hp: double, drat: double, wt: double, qsec: double, vs: double, am: double, gear: double, carb: double]
```
:::

:::{.fragment .fade-in}
```r

# Access things such a the PySpark functions directly
tbl_mtcars$corr("wt", "mpg")
#> [1] -0.8676594
```
:::
::: 

## Go further using *reticulate* {background-image="assets/slide-frame.png" background-size="1500px" background-color="white"}

Use `reticulate` to extend functionality

::: sample-large

```r
library(reticulate)
```

:::{.fragment .fade-in}
```r

# Load the Python libraries
pyspark <- import("pyspark")
connect_classification <- import("pyspark.ml.connect.classification")
```

:::

:::{.fragment .fade-in}
```r

# Build an array column that contains the feature variables
feature_fields <- pyspark$sql$functions$array("mpg", "wt")
tbl_features <- tbl_mtcars$withColumn("features", feature_fields)
```
:::

:::{.fragment .fade-in}
```r

# Instanciate a new model, and designate the label column
log_reg <- connect_classification$LogisticRegression()
log_reg_label <- log_reg$setLabelCol("am")
```
:::

:::

## Go further using *reticulate* {background-image="assets/slide-frame.png" background-size="1500px" background-color="white"}

Use `reticulate` to extend functionality

::: sample-large

```r
# Fit the model 
model <- log_reg_label$fit(tbl_features)
```

:::{.fragment .fade-in}
```r
model
#> LogisticRegression_144975e223ce
```
:::

:::{.fragment .fade-in}
```r
# Create the predictions
preds <- model$transform(tbl_features)
```
:::

:::{.fragment .fade-in}
```r
# Import data into R 
r_preds <- preds$toPandas()
```
:::
:::{.fragment .fade-in}
```r
head(r_preds, 3)
#>    mpg cyl disp  hp drat    wt  qsec vs am gear carb      features prediction          probability
#> 1 21.0   6  160 110 3.90 2.620 16.46  0  1    4    4   21.00, 2.62          1 0.4157636, 0.5842364
#> 2 21.0   6  160 110 3.90 2.875 17.02  0  1    4    4 21.000, 2.875          1 0.4890891, 0.5109109
#> 3 22.8   4  108  93 3.85 2.320 18.61  1  1    4    1   22.80, 2.32          1 0.2807752, 0.7192248
```
:::
:::

# In closing {background-image="assets/slide-frame-dark.png" background-size="1500px" background-color="#2a7070"}

:::{.dark-slide}
A quick review, and link sharing
:::

## Today, we covered{background-image="assets/slide-frame.png" background-size="1500px" background-color="#fff"}

-  <u>Spark Connect</u> enables remote connectivity to Spark clusters
-  Allows R users to interact with a cluster from their preferred environment
-  <u>Databricks Connect</u> , is based on **Spark Connect** architecture. Available in DBR version 13+
- Posit Workbench will support remote cluster management
- `sparklyr` supports Spark & Databricks Connect, via the `pysparklyr` extension
- The integration was developed using Python, and `PySpark`
- Access the entire `PySpark` API through `reticulate`


## Links{background-image="assets/slide-frame.png" background-size="1500px" background-color="#fff"}

- Spark Connect Overview - [https://spark.apache.org/docs/latest/spark-connect-overview.html](https://spark.apache.org/docs/latest/spark-connect-overview.html)
- Databricks Connect v2 intro post - [https://www.databricks.com/blog/2023/04/18/use-databricks-anywhere-databricks-connect-v2.html](https://www.databricks.com/blog/2023/04/18/use-databricks-anywhere-databricks-connect-v2.html)
- Using `sparklyr` with Connect article -  [https://spark.rstudio.com/deployment/databricks-spark-connect.html](https://spark.rstudio.com/deployment/databricks-spark-connect.html)

- Talk GitHub repo - [https://github.com/edgararuiz/talks/tree/main/databricks](https://github.com/edgararuiz/talks/tree/main/databricks)

## {background-image="assets/thank-you.png" background-size="1500px" background-color="black"}

![](assets/racoon.jpg){.absolute top=210 left=820 width=450}

## {background-image="assets/thank-you.png" background-size="1500px" background-color="black"}

<br>
<br>
<br>
<br>
<br>

:::{.dark-slide}
--------------- Link to the presentation ----->
:::
  
![](assets/qr-code.png){.absolute top=190 left=790 width=500}
